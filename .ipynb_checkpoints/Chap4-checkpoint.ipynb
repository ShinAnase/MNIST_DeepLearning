{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "#4章　ディープニューラルネットワーク\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ef90a6cb5336>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "#MNIST：0～9の手書き画像データ\n",
    "#枚数：55,000枚\n",
    "#解像度：28*28(=784)\n",
    "#PC:corei5,RAM 8GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "X = mnist.train.images\n",
    "Y = mnist.train.labels\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[0] #画像の枚数\n",
    "N = 100 #モデルの学習及び評価で使う枚数\n",
    "C = Y.shape[1] #クラス数（分類）\n",
    "indices = np.random.permutation(range(n))[:N] #ランダムにN枚選択。\n",
    "\n",
    "X_N = X[indices]\n",
    "Y_N = Y[indices]\n",
    "\n",
    "#train:test = 8:2でランダムに分割。\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_N, Y_N, train_size = 0.8) \n",
    "\n",
    "feature_size = X_N.shape[1] #特徴量の大きさ\n",
    "neuron_size = 200 #隠れ層のニューロンの数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29c2bc49f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ndarrayをgrayscaleの画像として表示\n",
    "plt.imshow(X[0].reshape(28, 28), cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#ニューラルネットワーク下準備(3層)\n",
    "x = tf.placeholder(tf.float32, shape = [None, feature_size]) #入力データ\n",
    "t = tf.placeholder(tf.float32, shape = [None, C]) #正解の出力(教師)\n",
    "\n",
    "#入力層-隠れ層\n",
    "#入力は2次元(重みWは特徴量×ニューロン数)、bはバイアス(ニューロン数×1)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "W = weight_variable([feature_size, neuron_size])\n",
    "b = bias_variable([neuron_size])\n",
    "h = tf.nn.relu(tf.matmul(x, W) + b)            #モデルの出力\n",
    "\n",
    "#隠れ層-出力層\n",
    "#入力は2次元(重みVはニューロン数×1)、bはバイアス(scalar)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "V = weight_variable([neuron_size, C])\n",
    "c = bias_variable([C])\n",
    "y = tf.nn.softmax(tf.matmul(h, V) + c)            #モデルの出力\n",
    "\n",
    "#交差エントロピー誤差関数(目的関数)\n",
    "#最小化させることで予想を行う。\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(t*tf.log(y), reduction_indices = [1]))\n",
    "\n",
    "#目的関数の偏微分と勾配降下法\n",
    "#最小化させることで予想を行う。\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) \n",
    "#0.1は学習率\n",
    "\n",
    "#modelの予測yと教師tが一致しているか(bool値)\n",
    "#argmax:一番高い確率を返す。\n",
    "correct_prediction = tf.equal(tf.argmax(y, axis = 1), tf.argmax(t, axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#モデルの学習\n",
    "\n",
    "epochs = 10000 #エポック数\n",
    "batch_size = 100 #ミニバッチサイズ\n",
    "n_batches = N//batch_size\n",
    "\n",
    "#変数(tf.Variable)の初期化\n",
    "init = tf.global_variables_initializer()\n",
    "#セッション(計算グラフ(学習)の実行環境)\n",
    "sess = tf.Session()\n",
    "#変数初期化の実行\n",
    "sess.run(init)\n",
    "\n",
    "#データの学習実行(ミニバッチ勾配降下法)\n",
    "for epoch in range(epochs): #各エポックごとにデータをシャッフル。\n",
    "    X_, Y_ = shuffle(X_train, Y_train)\n",
    "    for j in range(n_batches):\n",
    "        start = j * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict = {\n",
    "            x: X_[start:end],\n",
    "            t: Y_[start:end]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.65\n"
     ]
    }
   ],
   "source": [
    "#モデルの評価\n",
    "\n",
    "accuracy = correct_prediction.eval(session = sess, feed_dict = {\n",
    "    x: X_test,\n",
    "    t: Y_test\n",
    "})\n",
    "\n",
    "print(\"accuracy: \", accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output probability:\n",
      "[[1.00856483e-01 1.37456821e-03 2.24440685e-03 9.99072508e-05\n",
      "  2.57353787e-03 2.02345685e-03 8.70840073e-01 1.25420152e-03\n",
      "  1.83884185e-02 3.45017121e-04]\n",
      " [1.56812792e-04 4.31889566e-05 6.66469038e-01 2.08045053e-03\n",
      "  9.02806278e-07 1.06338135e-04 7.25982727e-06 3.11210930e-01\n",
      "  1.53856492e-03 1.83866434e-02]\n",
      " [1.08373744e-04 1.10265515e-04 9.76485848e-01 1.99610804e-04\n",
      "  3.31970587e-05 1.29047339e-05 2.12821923e-02 1.52112672e-03\n",
      "  2.24067888e-04 2.24723481e-05]\n",
      " [1.87978185e-05 3.70060443e-05 9.10758099e-05 6.61120866e-04\n",
      "  2.75828247e-03 2.10058097e-05 2.40361828e-06 9.92856264e-01\n",
      "  2.88318144e-04 3.26569122e-03]\n",
      " [2.19492722e-05 1.63513803e-04 4.11487272e-04 1.37939991e-03\n",
      "  9.63930607e-01 2.87341117e-03 9.73963324e-06 2.27409089e-03\n",
      "  1.45770116e-02 1.43588120e-02]\n",
      " [5.88923984e-04 1.20225914e-01 2.03940342e-03 6.25888724e-03\n",
      "  1.09464012e-01 6.96778893e-02 2.94455271e-02 1.26511871e-03\n",
      "  2.38274131e-03 6.58651590e-01]\n",
      " [6.50130687e-07 1.84358796e-04 2.76025065e-04 4.49534491e-06\n",
      "  1.22841122e-02 1.14326476e-05 6.99730456e-01 2.81821609e-01\n",
      "  3.93028464e-03 1.75652106e-03]\n",
      " [3.67838311e-06 5.61905850e-04 2.72812940e-05 9.98184383e-01\n",
      "  4.36709252e-06 5.00431401e-04 7.71224450e-07 1.93972446e-05\n",
      "  6.85392937e-04 1.23923528e-05]\n",
      " [4.95049846e-07 1.95003478e-04 1.14154045e-05 9.99491572e-01\n",
      "  2.63210495e-07 1.48767504e-04 9.37330606e-06 1.89480744e-07\n",
      "  1.40909484e-04 1.77998243e-06]\n",
      " [9.27779853e-01 1.20459241e-03 8.01662821e-03 4.03936207e-02\n",
      "  3.47478228e-04 4.84906416e-03 9.05661378e-03 9.48575267e-04\n",
      "  7.07144570e-03 3.32079624e-04]\n",
      " [6.65346107e-08 7.61234098e-09 3.27300472e-06 2.65618763e-08\n",
      "  9.99894738e-01 3.00480309e-07 8.67494805e-07 4.94060769e-05\n",
      "  2.19703215e-05 2.94254569e-05]\n",
      " [6.94601977e-06 9.74872708e-01 3.68705987e-05 1.83665641e-02\n",
      "  1.29596010e-04 8.00120833e-05 8.67405397e-05 6.26909640e-03\n",
      "  3.82074468e-05 1.13406124e-04]\n",
      " [2.30339557e-01 1.62525961e-04 8.66602436e-02 1.66909122e-05\n",
      "  3.65577784e-04 1.02910865e-03 6.59619927e-01 3.83993523e-04\n",
      "  2.13549789e-02 6.74640469e-05]\n",
      " [9.99949098e-01 9.84186510e-10 9.28889179e-08 4.61406984e-08\n",
      "  4.99891404e-12 1.27732324e-06 4.91303690e-05 4.73083528e-09\n",
      "  1.12764930e-07 2.50430332e-07]\n",
      " [3.13431304e-03 2.17115805e-02 1.12744165e-03 1.34481220e-02\n",
      "  1.88565219e-03 2.98127113e-03 5.72010526e-04 5.18038578e-04\n",
      "  9.54554617e-01 6.68641223e-05]]\n"
     ]
    }
   ],
   "source": [
    "#入力（X)を食わせた時の出力\n",
    "prob = y.eval(session = sess, feed_dict = {\n",
    "    x: X_test\n",
    "})\n",
    "\n",
    "print(\"output probability:\")\n",
    "print(prob[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像1000枚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0] #画像の枚数\n",
    "N = 1000 #モデルの学習及び評価で使う枚数\n",
    "C = Y.shape[1] #クラス数（分類）\n",
    "indices = np.random.permutation(range(n))[:N] #ランダムにN枚選択。\n",
    "\n",
    "X_N = X[indices]\n",
    "Y_N = Y[indices]\n",
    "\n",
    "#train:test = 8:2でランダムに分割。\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_N, Y_N, train_size = 0.8) \n",
    "\n",
    "feature_size = X_N.shape[1] #特徴量の大きさ\n",
    "neuron_size = 200 #隠れ層のニューロンの数\n",
    "\n",
    "#ニューラルネットワーク下準備(3層)\n",
    "x = tf.placeholder(tf.float32, shape = [None, feature_size]) #入力データ\n",
    "t = tf.placeholder(tf.float32, shape = [None, C]) #正解の出力(教師)\n",
    "\n",
    "#入力層-隠れ層\n",
    "#入力は2次元(重みWは特徴量×ニューロン数)、bはバイアス(ニューロン数×1)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "W = weight_variable([feature_size, neuron_size])\n",
    "b = bias_variable([neuron_size])\n",
    "h = tf.nn.relu(tf.matmul(x, W) + b)            #モデルの出力\n",
    "\n",
    "#隠れ層-出力層\n",
    "#入力は2次元(重みVはニューロン数×1)、bはバイアス(scalar)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "V = weight_variable([neuron_size, C])\n",
    "c = bias_variable([C])\n",
    "y = tf.nn.softmax(tf.matmul(h, V) + c)            #モデルの出力\n",
    "\n",
    "#交差エントロピー誤差関数(目的関数)\n",
    "#最小化させることで予想を行う。\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(t*tf.log(y), reduction_indices = [1]))\n",
    "\n",
    "#目的関数の偏微分と勾配降下法\n",
    "#最小化させることで予想を行う。\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) \n",
    "#0.1は学習率\n",
    "\n",
    "#modelの予測yと教師tが一致しているか(bool値)\n",
    "#argmax:一番高い確率を返す。\n",
    "correct_prediction = tf.equal(tf.argmax(y, axis = 1), tf.argmax(t, axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#モデルの学習\n",
    "\n",
    "epochs = 10000 #エポック数\n",
    "batch_size = 100 #ミニバッチサイズ\n",
    "n_batches = N//batch_size\n",
    "\n",
    "#変数(tf.Variable)の初期化\n",
    "init = tf.global_variables_initializer()\n",
    "#セッション(計算グラフ(学習)の実行環境)\n",
    "sess = tf.Session()\n",
    "#変数初期化の実行\n",
    "sess.run(init)\n",
    "\n",
    "#データの学習実行(ミニバッチ勾配降下法)\n",
    "for epoch in range(epochs): #各エポックごとにデータをシャッフル。\n",
    "    X_, Y_ = shuffle(X_train, Y_train)\n",
    "    for j in range(n_batches):\n",
    "        start = j * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict = {\n",
    "            x: X_[start:end],\n",
    "            t: Y_[start:end]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.88\n"
     ]
    }
   ],
   "source": [
    "#モデルの評価\n",
    "\n",
    "accuracy = correct_prediction.eval(session = sess, feed_dict = {\n",
    "    x: X_test,\n",
    "    t: Y_test\n",
    "})\n",
    "\n",
    "print(\"accuracy: \", accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像10000枚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0] #画像の枚数\n",
    "N = 10000 #モデルの学習及び評価で使う枚数\n",
    "C = Y.shape[1] #クラス数（分類）\n",
    "indices = np.random.permutation(range(n))[:N] #ランダムにN枚選択。\n",
    "\n",
    "X_N = X[indices]\n",
    "Y_N = Y[indices]\n",
    "\n",
    "#train:test = 8:2でランダムに分割。\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_N, Y_N, train_size = 0.8) \n",
    "\n",
    "feature_size = X_N.shape[1] #特徴量の大きさ\n",
    "neuron_size = 200 #隠れ層のニューロンの数\n",
    "\n",
    "#ニューラルネットワーク下準備(3層)\n",
    "x = tf.placeholder(tf.float32, shape = [None, feature_size]) #入力データ\n",
    "t = tf.placeholder(tf.float32, shape = [None, C]) #正解の出力(教師)\n",
    "\n",
    "#入力層-隠れ層\n",
    "#入力は2次元(重みWは特徴量×ニューロン数)、bはバイアス(ニューロン数×1)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "W = weight_variable([feature_size, neuron_size])\n",
    "b = bias_variable([neuron_size])\n",
    "h = tf.nn.relu(tf.matmul(x, W) + b)            #モデルの出力\n",
    "\n",
    "#隠れ層-出力層\n",
    "#入力は2次元(重みVはニューロン数×1)、bはバイアス(scalar)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "V = weight_variable([neuron_size, C])\n",
    "c = bias_variable([C])\n",
    "y = tf.nn.softmax(tf.matmul(h, V) + c)            #モデルの出力\n",
    "\n",
    "#交差エントロピー誤差関数(目的関数)\n",
    "#最小化させることで予想を行う。\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(t*tf.log(y), reduction_indices = [1]))\n",
    "\n",
    "#目的関数の偏微分と勾配降下法\n",
    "#最小化させることで予想を行う。\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) \n",
    "#0.1は学習率\n",
    "\n",
    "#modelの予測yと教師tが一致しているか(bool値)\n",
    "#argmax:一番高い確率を返す。\n",
    "correct_prediction = tf.equal(tf.argmax(y, axis = 1), tf.argmax(t, axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#モデルの学習\n",
    "\n",
    "epochs = 10000 #エポック数\n",
    "batch_size = 100 #ミニバッチサイズ\n",
    "n_batches = N//batch_size\n",
    "\n",
    "#変数(tf.Variable)の初期化\n",
    "init = tf.global_variables_initializer()\n",
    "#セッション(計算グラフ(学習)の実行環境)\n",
    "sess = tf.Session()\n",
    "#変数初期化の実行\n",
    "sess.run(init)\n",
    "\n",
    "#データの学習実行(ミニバッチ勾配降下法)\n",
    "for epoch in range(epochs): #各エポックごとにデータをシャッフル。\n",
    "    X_, Y_ = shuffle(X_train, Y_train)\n",
    "    for j in range(n_batches):\n",
    "        start = j * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict = {\n",
    "            x: X_[start:end],\n",
    "            t: Y_[start:end]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.093\n"
     ]
    }
   ],
   "source": [
    "#モデルの評価\n",
    "\n",
    "accuracy = correct_prediction.eval(session = sess, feed_dict = {\n",
    "    x: X_test,\n",
    "    t: Y_test\n",
    "})\n",
    "\n",
    "print(\"accuracy: \", accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000枚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\hfuis\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[0] #画像の枚数\n",
    "N = 5000 #モデルの学習及び評価で使う枚数\n",
    "C = Y.shape[1] #クラス数（分類）\n",
    "indices = np.random.permutation(range(n))[:N] #ランダムにN枚選択。\n",
    "\n",
    "X_N = X[indices]\n",
    "Y_N = Y[indices]\n",
    "\n",
    "#train:test = 8:2でランダムに分割。\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_N, Y_N, train_size = 0.8) \n",
    "\n",
    "feature_size = X_N.shape[1] #特徴量の大きさ\n",
    "neuron_size = 200 #隠れ層のニューロンの数\n",
    "\n",
    "#ニューラルネットワーク下準備(3層)\n",
    "x = tf.placeholder(tf.float32, shape = [None, feature_size]) #入力データ\n",
    "t = tf.placeholder(tf.float32, shape = [None, C]) #正解の出力(教師)\n",
    "\n",
    "#入力層-隠れ層\n",
    "#入力は2次元(重みWは特徴量×ニューロン数)、bはバイアス(ニューロン数×1)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "W = weight_variable([feature_size, neuron_size])\n",
    "b = bias_variable([neuron_size])\n",
    "h = tf.nn.relu(tf.matmul(x, W) + b)            #モデルの出力\n",
    "\n",
    "#隠れ層-出力層\n",
    "#入力は2次元(重みVはニューロン数×1)、bはバイアス(scalar)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "V = weight_variable([neuron_size, C])\n",
    "c = bias_variable([C])\n",
    "y = tf.nn.softmax(tf.matmul(h, V) + c)            #モデルの出力\n",
    "\n",
    "#交差エントロピー誤差関数(目的関数)\n",
    "#最小化させることで予想を行う。\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(t*tf.log(y), reduction_indices = [1]))\n",
    "\n",
    "#目的関数の偏微分と勾配降下法\n",
    "#最小化させることで予想を行う。\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) \n",
    "#0.1は学習率\n",
    "\n",
    "#modelの予測yと教師tが一致しているか(bool値)\n",
    "#argmax:一番高い確率を返す。\n",
    "correct_prediction = tf.equal(tf.argmax(y, axis = 1), tf.argmax(t, axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#モデルの学習\n",
    "\n",
    "epochs = 10000 #エポック数\n",
    "batch_size = 100 #ミニバッチサイズ\n",
    "n_batches = N//batch_size\n",
    "\n",
    "#変数(tf.Variable)の初期化\n",
    "init = tf.global_variables_initializer()\n",
    "#セッション(計算グラフ(学習)の実行環境)\n",
    "sess = tf.Session()\n",
    "#変数初期化の実行\n",
    "sess.run(init)\n",
    "\n",
    "#データの学習実行(ミニバッチ勾配降下法)\n",
    "for epoch in range(epochs): #各エポックごとにデータをシャッフル。\n",
    "    X_, Y_ = shuffle(X_train, Y_train)\n",
    "    for j in range(n_batches):\n",
    "        start = j * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict = {\n",
    "            x: X_[start:end],\n",
    "            t: Y_[start:end]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.109\n"
     ]
    }
   ],
   "source": [
    "#モデルの評価\n",
    "\n",
    "accuracy = correct_prediction.eval(session = sess, feed_dict = {\n",
    "    x: X_test,\n",
    "    t: Y_test\n",
    "})\n",
    "\n",
    "print(\"accuracy: \", accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3000枚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0] #画像の枚数\n",
    "N = 3000 #モデルの学習及び評価で使う枚数\n",
    "C = Y.shape[1] #クラス数（分類）\n",
    "indices = np.random.permutation(range(n))[:N] #ランダムにN枚選択。\n",
    "\n",
    "X_N = X[indices]\n",
    "Y_N = Y[indices]\n",
    "\n",
    "#train:test = 8:2でランダムに分割。\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_N, Y_N, train_size = 0.8) \n",
    "\n",
    "feature_size = X_N.shape[1] #特徴量の大きさ\n",
    "neuron_size = 200 #隠れ層のニューロンの数\n",
    "\n",
    "#ニューラルネットワーク下準備(3層)\n",
    "x = tf.placeholder(tf.float32, shape = [None, feature_size]) #入力データ\n",
    "t = tf.placeholder(tf.float32, shape = [None, C]) #正解の出力(教師)\n",
    "\n",
    "#入力層-隠れ層\n",
    "#入力は2次元(重みWは特徴量×ニューロン数)、bはバイアス(ニューロン数×1)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "W = weight_variable([feature_size, neuron_size])\n",
    "b = bias_variable([neuron_size])\n",
    "h = tf.nn.relu(tf.matmul(x, W) + b)            #モデルの出力\n",
    "\n",
    "#隠れ層-出力層\n",
    "#入力は2次元(重みVはニューロン数×1)、bはバイアス(scalar)。\n",
    "#truncated_normal:切断正規分布(defaultだとmean=0,std=1)\n",
    "V = weight_variable([neuron_size, C])\n",
    "c = bias_variable([C])\n",
    "y = tf.nn.softmax(tf.matmul(h, V) + c)            #モデルの出力\n",
    "\n",
    "#交差エントロピー誤差関数(目的関数)\n",
    "#最小化させることで予想を行う。\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(t*tf.log(y), reduction_indices = [1]))\n",
    "\n",
    "#目的関数の偏微分と勾配降下法\n",
    "#最小化させることで予想を行う。\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) \n",
    "#0.1は学習率\n",
    "\n",
    "#modelの予測yと教師tが一致しているか(bool値)\n",
    "#argmax:一番高い確率を返す。\n",
    "correct_prediction = tf.equal(tf.argmax(y, axis = 1), tf.argmax(t, axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#モデルの学習\n",
    "\n",
    "epochs = 10000 #エポック数\n",
    "batch_size = 100 #ミニバッチサイズ\n",
    "n_batches = N//batch_size\n",
    "\n",
    "#変数(tf.Variable)の初期化\n",
    "init = tf.global_variables_initializer()\n",
    "#セッション(計算グラフ(学習)の実行環境)\n",
    "sess = tf.Session()\n",
    "#変数初期化の実行\n",
    "sess.run(init)\n",
    "\n",
    "#データの学習実行(ミニバッチ勾配降下法)\n",
    "for epoch in range(epochs): #各エポックごとにデータをシャッフル。\n",
    "    X_, Y_ = shuffle(X_train, Y_train)\n",
    "    for j in range(n_batches):\n",
    "        start = j * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict = {\n",
    "            x: X_[start:end],\n",
    "            t: Y_[start:end]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.09666666666666666\n"
     ]
    }
   ],
   "source": [
    "#モデルの評価\n",
    "\n",
    "accuracy = correct_prediction.eval(session = sess, feed_dict = {\n",
    "    x: X_test,\n",
    "    t: Y_test\n",
    "})\n",
    "\n",
    "print(\"accuracy: \", accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output probability:\n",
      "[[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "#入力（X)を食わせた時の出力\n",
    "prob = y.eval(session = sess, feed_dict = {\n",
    "    x: X_test\n",
    "})\n",
    "\n",
    "print(\"output probability:\")\n",
    "print(prob[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
